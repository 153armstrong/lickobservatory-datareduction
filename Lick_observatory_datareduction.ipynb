{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize all the directories "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.io import fits,ascii\n",
    "import numpy as np\n",
    "import sys, getopt,os\n",
    "import glob2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_dir = '/home/keerthi/Documents/LICK_workshop/DataReduction_2/Nickel_data/'\n",
    "\n",
    "\n",
    "#Location of the folder containing all the data files (bias, domeflats, twilight flats and the data files)\n",
    "f = source_dir + '**/*.fits'\n",
    "\n",
    "#Loation of the bias folder and files\n",
    "f1 = source_dir + '2018.10.14-Nickel/bias/*.fits'\n",
    "b_file = source_dir + '2018.10.14-Nickel/flat_dome/b/**/*.fits'\n",
    "v_file = source_dir + '2018.10.14-Nickel/flat_dome/v/**/*.fits'\n",
    "r_file = source_dir + '2018.10.14-Nickel/flat_dome/r/**/*.fits'\n",
    "i_file = source_dir + '2018.10.14-Nickel/flat_dome/i/**/*.fits'\n",
    "\n",
    "#Location of the data files\n",
    "f2 = source_dir +'2018.10.14-Nickel/Data_files/**/*.fits'\n",
    "f3 = source_dir + '2018.10.14-Nickel/Data_files/'\n",
    "f4 = f2\n",
    "\n",
    "#Location of output folder of the reduced images\n",
    "output_folder1 = source_dir + '2018.10.14-Nickel/'\n",
    "output_folder_2 = output_folder1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overscan Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: AstropyDeprecationWarning: \"clobber\" was deprecated in version 2.0 and will be removed in a future version. Use argument \"overwrite\" instead. [__main__]\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "# Version 2.3 -- Keerthi Vasan 2018 Dec 22 - Adapted it for Jupyter-notebook \n",
    "# Version 2.2 -- Elinor Gates, 2018 Oct 3  - changed clobber to overwrite in fits.writeto\n",
    "# Version 2.1 -- Elinor Gates, 2016 Aug 25 - changed math to BITPIX -32\n",
    "# Version 2.0 -- Elinor Gates, 2016 Aug 11 - improved file reading\n",
    "# Version 1.0 -- Elinor Gates, 2015 Nov 24\n",
    "\n",
    "def main(argv):\n",
    "    inputfilelist = ''\n",
    "    outputfilelist = ''\n",
    "    fit = 'no'\n",
    "    try:\n",
    "        opts, args = getopt.getopt(argv,\"hfi:o:\",[\"ifilelist=\",\"ofilelist=\"])\n",
    "    except getopt.GetoptError:\n",
    "#       print 'overscanLickObs.py -f -i <inputfilelist> -o <outputfilelist>'\n",
    "#       print '-f indicates do a Legendre fit to overscan'\n",
    "      sys.exit(2)\n",
    "    for opt, arg in opts:\n",
    "      if opt == '-h':\n",
    "#          print 'overscanLickObs.py -f -i <inputfilelist> -o <outputfilelist>'\n",
    "#          print '-f indicates do a Legendre fit to overscan'\n",
    "         sys.exit(2)\n",
    "      elif opt in (\"-i\", \"--ifilelist\"):\n",
    "         inputfilelist = arg\n",
    "      elif opt in (\"-o\", \"--ofilelist\"):\n",
    "         outputfilelist = arg\n",
    "      elif opt == '-f':\n",
    "          fit = 'yes' \n",
    "#     print 'Input filelist is ', inputfilelist\n",
    "#     print 'Output filelist is ', outputfilelist\n",
    "#     print 'Fit is ', fit\n",
    "\n",
    "    # open input and output filelists\n",
    "    ifilelist = glob2.glob(f)\n",
    "    ofilelist = [i[:-5]+ '_os.fits' for i in ifilelist]\n",
    "    \n",
    "    # how many files\n",
    "    numifiles = len(ifilelist)\n",
    "    numofiles = len(ofilelist)\n",
    "    if numifiles != numofiles:\n",
    "        sys.exit('Input and output file lists have different numbers of files. Exiting.')\n",
    "\n",
    "    # for each file in ifilelist, read in file, figure out overscan and data regions, fit \n",
    "    # overscan with desired function (if any), and subtract from data.  \n",
    "    # Write data to ofilelist value.  \n",
    "\n",
    "    for i in range(0,numifiles):\n",
    "        ifile=ifilelist[i]\n",
    "        ofile=ofilelist[i]\n",
    "        data, header = fits.getdata(ifile,header=True)\n",
    "\n",
    "        # change data to float\n",
    "        data=data.astype('float32')\n",
    "\n",
    "        # read necessary keywords from fits header\n",
    "        xsize = header['NAXIS1']\n",
    "        ysize = header['NAXIS2']\n",
    "        xorig = header['CRVAL1U']\n",
    "        yorig = header['CRVAL2U']\n",
    "        cdelt1 = header['CDELT1U']\n",
    "        cdelt2 = header['CDELT2U']\n",
    "        rover = header['ROVER']\n",
    "        cover = header['COVER']\n",
    "        inxsize = header['DNAXIS1']\n",
    "        inysize = header['DNAXIS2']\n",
    "        ampsx = header['AMPSCOL']\n",
    "        ampsy = header['AMPSROW']\n",
    "\n",
    "        # determine number and sizes of overscan and data regions\n",
    "        namps = ampsx*ampsy\n",
    "        if rover > 0:\n",
    "            over=rover\n",
    "            sys.exit('Program does not yet deal with row overscans. Exiting.')\n",
    "        else:\n",
    "            over = cover\n",
    "        if over == 0:\n",
    "            sys.exit('No overscan region specified in FITS header. Exiting.')\n",
    "\n",
    "        # single amplifier mode\n",
    "        if namps == 1:\n",
    "            biassec = data[:,xsize-cover:xsize]\n",
    "            datasec = data[0:,0:xsize-cover]\n",
    "\n",
    "            # median overscan section\n",
    "            bias=np.median(biassec, axis=1) \n",
    "\n",
    "            # legendre fit\n",
    "            if fit == 'yes':\n",
    "                # fit\n",
    "                lfit = np.polynomial.legendre.legfit(range(0,len(bias)),bias,3)\n",
    "                bias = np.polynomial.legendre.legval(range(0,len(bias)),lfit)\n",
    "\n",
    "            # subtract overscan\n",
    "            datanew = datasec\n",
    "            for i in range(datasec.shape[1]):\n",
    "                datanew[:,i] = datasec[:,i]-bias\n",
    "\n",
    "        # two amplifier mode\n",
    "        if namps == 2:\n",
    "            biasseca = data[:,xsize-cover*2:xsize-cover]\n",
    "            biassecb = data[:,xsize-cover:xsize]\n",
    "\n",
    "            # median overscan sections\n",
    "            biasa=np.median(biasseca,axis=1)\n",
    "            biasb=np.median(biassecb,axis=1)\n",
    "\n",
    "            # legendre fit\n",
    "            if fit == 'yes':\n",
    "                lfita = np.polynomial.legendre.legfit(range(0,len(biasa)),biasa,3)\n",
    "                lfitb = np.polynomial.legendre.legfit(range(0,len(biasb)),biasb,3)\n",
    "                biasa = np.polynomial.legendre.legval(range(0,len(biasa)),lfita)\n",
    "                biasb = np.polynomial.legendre.legval(range(0,len(biasb)),lfitb)\n",
    "\n",
    "            # extract data regions\n",
    "\n",
    "            #determine size of binned data region\n",
    "            hsize=abs(inxsize/cdelt1)\n",
    " \n",
    "            # calculate x origin of readout in binned units if cdelt1 negative or positive\n",
    "            if cdelt1 < 0:\n",
    "                xorig=(xorig-(xsize-2*cover)*abs(cdelt1))/abs(cdelt1)\n",
    "            else:\n",
    "                xorig=xorig/cdelt1\n",
    "            x0=xorig+xsize-1-cover*2 # need to test is need -1 because starting counting at 0\n",
    "\n",
    "            # determine which columns are on which amplifier and subtract proper overscan region\n",
    "\n",
    "            if x0 < hsize/2: # all data on left amplifier\n",
    "                datanew=data[:,0:xsize-cover*2]\n",
    "                m=datanew.shape[1]\n",
    "                for i in range(0,m):\n",
    "                    datanew[:,i]=datanew[:,i]-biasa\n",
    "\n",
    "            if xorig >= hsize/2: # all data on right amplifier\n",
    "                datanew=data[:,0:xsize-cover*2]\n",
    "                m=datanew.shape[1]\n",
    "                for i in range(0,m):\n",
    "                    datanew[:,i]=datanew[:,i]-biasb\n",
    "\n",
    "            if xorig < hsize/2 and x0 > hsize/2:\n",
    "                x1=hsize/2-xorig\n",
    "                dataa=data[:,0:x1]\n",
    "                datab=data[:,x1:-cover*2]\n",
    "                ma=dataa.shape[1]\n",
    "                mb=datab.shape[1]\n",
    "                for i in range(0,ma):\n",
    "                    dataa[:,i]=dataa[:,i]-biasa\n",
    "                for i in range(0,mb):\n",
    "                    datab[:,i]=datab[:,i]-biasb\n",
    "                # merge dataa and datab into single image\n",
    "                datanew=np.hstack([dataa,datab])\n",
    "\n",
    "        if namps > 2: \n",
    "            sys.exit('Program does not yet deal with more than two overscan regions. Exiting.')\n",
    "\n",
    "        # add info to header\n",
    "        header['HISTORY'] = 'Overscan subtracted'\n",
    "\n",
    "        # write new fits file\n",
    "        fits.writeto(ofile,datanew,header,clobber=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "   main(sys.argv[1:])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "biasfiles = glob2.glob(f1)\n",
    "data_stack = []\n",
    "for file in biasfiles:\n",
    "    data_stack.append(fits.getdata(file))\n",
    "    \n",
    "### Generate the bias file ###\n",
    "\n",
    "medianBias = np.median(data_stack,axis=0)\n",
    "header = fits.getheader(biasfiles[0])\n",
    "header['HISTORY'] = 'Median combined'\n",
    "fits.writeto(output_folder1+ 'bias.fits',medianBias,header)\n",
    "\n",
    "for file in biasfiles:\n",
    "    os.remove(file)\n",
    "    \n",
    "datafilesin = glob2.glob(f2)\n",
    "datafilesout = [i[:-5]+ '_bs.fits' for i in datafilesin]\n",
    "n = len(datafilesin)\n",
    "for i in range(0,n):\n",
    "    data,header = fits.getdata(datafilesin[i],header=True)\n",
    "    dataout = data - medianBias\n",
    "    header['HISTORY'] = 'Bias subtracted'\n",
    "    fits.writeto(datafilesout[i],dataout,header)\n",
    "    os.remove(datafilesin[i])\n",
    "\n",
    "    \n",
    "### Bias Subtraction from the b-filter ###\n",
    "bflatfiles =  glob2.glob(b_file)\n",
    "bflat_stack = []\n",
    "for file in bflatfiles:\n",
    "    data,header = fits.getdata(file,header=True)\n",
    "    data = data / np.median(data)\n",
    "    bflat_stack.append(data)\n",
    "    os.remove(file)\n",
    "\n",
    "bflat = np.median(bflat_stack,axis=0)\n",
    "m = np.mean(bflat)\n",
    "bflat = bflat/m\n",
    "header['HISTORY'] = 'Combined and normalized flat field'\n",
    "fits.writeto(output_folder_2 + 'bflat.fits',bflat,header)\n",
    "\n",
    "\n",
    "\n",
    "### Bias Subtraction from the v-filter ###\n",
    "\n",
    "vflatfiles =  glob2.glob(v_file)\n",
    "vflat_stack = []\n",
    "for file in vflatfiles:\n",
    "    data,header = fits.getdata(file,header=True)\n",
    "    data = data / np.median(data)\n",
    "    vflat_stack.append(data)\n",
    "    os.remove(file)\n", ##
    "\n",
    "vflat = np.median(vflat_stack,axis=0)\n",
    "m1 = np.mean(vflat)\n",
    "vflat = vflat/m1\n",
    "\n",
    "header['HISTORY'] = 'Combined and normalized flat field'\n",
    "fits.writeto(output_folder_2 + 'vflat.fits',vflat,header)\n",
    "\n",
    "\n",
    "### Bias Subtraction from the r-filter ###\n",
    "\n",
    "rflatfiles =  glob2.glob(r_file)\n",
    "rflat_stack = []\n",
    "for file in rflatfiles:\n",
    "    data,header = fits.getdata(file,header=True)\n",
    "    data = data / np.median(data)\n",
    "    rflat_stack.append(data)\n",
    "    os.remove(file)\n",
    "\n",
    "rflat = np.median(rflat_stack,axis=0)\n",
    "m2 = np.mean(rflat)\n",
    "rflat = rflat/m2\n",
    "\n",
    "header['HISTORY'] = 'Combined and normalized flat field'\n",
    "fits.writeto(output_folder_2 +'rflat.fits',rflat,header)\n",
    "\n",
    "\n",
    "### Bias Subtraction from the i-filter ###\n",
    "\n",
    "iflatfiles =  glob2.glob(i_file)\n",
    "iflat_stack = []\n",
    "for file in iflatfiles:\n",
    "    data,header = fits.getdata(file,header=True)\n",
    "    data = data / np.median(data)\n",
    "    iflat_stack.append(data)\n",
    "    os.remove(file)\n",
    "\n",
    "iflat = np.median(iflat_stack,axis=0)\n",
    "m3 = np.mean(iflat)\n",
    "iflat = iflat/m3\n",
    "\n",
    "header['HISTORY'] = 'Combined and normalized flat field'\n",
    "fits.writeto(output_folder_2 + 'iflat.fits',iflat,header)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flat field correction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Flat field correction in the b-filter ###\n",
    "\n",
    "bdatain =  glob2.glob(f3+ 'b/**/*.fits')\n",
    "bdataout = [i[:-5]+ '_ff.fits' for i in bdatain]\n",
    "n=len(bdatain)\n",
    "for i in range(0,n):\n",
    "    data,header = fits.getdata(bdatain[i],header=True)\n",
    "    dataout = data / bflat\n",
    "    header['HISTORY'] = 'Flat Fielded'\n",
    "    fits.writeto(bdataout[i],dataout,header)\n",
    "    os.remove(bdatain[i])\n",
    "    \n",
    "    \n",
    "### Flat field correction in the v-filter ###\n",
    "vdatain =  glob2.glob(f3+ 'v/**/*.fits')\n",
    "vdataout = [i[:-5]+ '_ff.fits' for i in vdatain]\n",
    "\n",
    "n1=len(vdatain)\n",
    "for i in range(0,n1):\n",
    "    data,header = fits.getdata(vdatain[i],header=True)\n",
    "    dataout1 = data / vflat\n",
    "    header['HISTORY'] = 'Flat Fielded'\n",
    "    fits.writeto(vdataout[i],dataout1,header)\n",
    "    os.remove(vdatain[i])\n",
    "    \n",
    "\n",
    "### Flat field correction in the r-filter ###\n",
    "rdatain =  glob2.glob(f3+ 'r/**/*.fits')\n",
    "rdataout = [i[:-5]+ '_ff.fits' for i in rdatain]\n",
    "\n",
    "n2=len(rdatain)\n",
    "for i in range(0,n2):\n",
    "    data,header = fits.getdata(rdatain[i],header=True)\n",
    "    dataout2 = data / rflat\n",
    "    header['HISTORY'] = 'Flat Fielded'\n",
    "    fits.writeto(rdataout[i],dataout2,header)\n",
    "    os.remove(rdatain[i])\n",
    "    \n",
    "\n",
    "### Flat field correction in the i-filter ###\n",
    "idatain =  glob2.glob(f3+ 'i/**/*.fits')\n",
    "idataout = [i[:-5]+ '_ff.fits' for i in idatain]\n",
    "\n",
    "n3=len(idatain)\n",
    "for i in range(0,n3):\n",
    "    data,header = fits.getdata(idatain[i],header=True)\n",
    "    dataout3 = data / iflat\n",
    "    header['HISTORY'] = 'Flat Fielded'\n",
    "    fits.writeto(idataout[i],dataout3,header)    \n",
    "    os.remove(idatain[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keerthi/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:31: UserWarning: Warning: converting a masked element to nan.\n"
     ]
    }
   ],
   "source": [
    "# Procedure to fix known bad columns in CCD2 images.  2016 Oct 2 E. Gates\n",
    "datain =  glob2.glob(f4)\n",
    "dataout = [i[:-5]+ '_FINAL.fits' for i in datain]\n",
    "\n",
    "n=len(datain)\n",
    "# size of box for area around bad pixel to be averaged\n",
    "s=2\n",
    "\n",
    "#read in one image to get image size for bad pixel mask\n",
    "data,header=fits.getdata(datain[0],header=True)\n",
    "\n",
    "#make bad pixel mask\n",
    "mask=np.ma.make_mask(data,copy=True,shrink=True,dtype=np.bool)\n",
    "mask[:,:]=False\n",
    "mask[:,255:257]=True\n",
    "mask[:,783:785]=True\n",
    "mask[:,1001:1003]=True\n",
    "\n",
    "#loop for all the data bad pixel correction \n",
    "for k in range(0,n):\n",
    "  data,header=fits.getdata(datain[k],header=True)\n",
    "  mdata=np.ma.masked_array(data,mask=mask,fill_value=np.nan)\n",
    "  dataFixed=data.copy()\n",
    "  for i in range(0,mdata.shape[0]):\n",
    "    for j in range(0,mdata.shape[1]):\n",
    "      if np.math.isnan(mdata[i,j]):\n",
    "        x1=i-s\n",
    "        x2=i+s+1\n",
    "        y1=j-s\n",
    "        y2=j+s+1\n",
    "        if x1<0:\n",
    "          x1=0\n",
    "        if x2>mdata.shape[0]:\n",
    "          x2=mdata.shape[0]\n",
    "        if y1<0:\n",
    "          y1=0\n",
    "        if y2>mdata.shape[1]:\n",
    "          y2=mdata.shape[1]\n",
    "        dataFixed[i,j]=np.mean(mdata[x1:x2,y1:y2])\n",
    "  header['HISTORY']='Bad columns replaced'\n",
    "  fits.writeto(dataout[k],dataFixed,header,overwrite=True)\n",
    "  os.remove(datain[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have reached the end of the pipeline where we have performed overscan subtraction, bias subtraction, Flat field correction, removed the bad pixels in the CCD and saved the data files into the folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
